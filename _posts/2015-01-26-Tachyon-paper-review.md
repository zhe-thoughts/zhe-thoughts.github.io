---
layout: post
title: Paper review: Tachyon (SoCC '14)
---
## Summary
[Tachyon](https://amplab.cs.berkeley.edu/publication/tachyon-socc/) is a distributed caching layer on top of disk-based file systems such as HDFS or GlusterFS. The core idea is to use lineage to provide **lightweight, optimistic fault tolerance**. 

> More importantly, due to the inherent bandwidth limitations of replication, a lineage-based recovery model might be the _only_ way to make cluster storage systems match the speed of in-memory computations in the future. 

Extending from "[Spark](http://spark.apache.org/research.html)", Tachyon also assumes an elegant *Lambda-calculus-flavored* model, where new datasets are generated by closed-form operators (e.g., _map_ and _group-by_) from existing ones. This empowers Spark and Tachyon to persist the operator binaries -- whose sizes are negligible -- instead of replicating the datasets themselves. As quoted above, this might indeed be the _only_ way to write data with local memory speed -- from an information theoretic perspective, how else could cheap redundancy be achieved?

Under this elegant assumption, Spark allows you to program with PB-sized variables; and Tachyon provides a name space for those variables to be shared among applications. The programming model becomes much more flexible than _MapReduce_.

Tachyon's API is quite simple:

| Signature        | Return |
| ------------- |:-------------:|
| createDependency(inputFiles, outputFiles, binaryPrograms, config, dependencyType)     | lineage ID |
| getDependency(lineageId)     | Dependency Info      |


## Limitations
Tachyon should work well as long as the basic assumption holds: datasets are connected by closed-form *jobs*. In general, this should hold for most analytical workloads. But how about transactions, like HBase? The *job* binary -- describing a newly inserted value -- will be as large as the data itself.

## My assessment
The initial idea of Spark was similar to [MapReduce Online](https://code.google.com/p/hop/). However, the lineage-based optimistic fault tolerance model greatly generalized and *externalized* inter-job intermediate results, bringing them to the unprecedented stage of programmable units. This is a breakthrough in distributed computing.

Tachyon itself makes 2 new contributions, as outlined in the abstract:

> The key challenge in making a long-running lineage-based storage system is timely data recovery in case of failures. Tachyon addresses this issue by introducing a *checkpointing algorithm* that guarantees bounded recovery cost and *resource allocation strategies* for recomputation under commonly used resource schedulers.

Both are solid, incremental contributions around better *prioritization* of fault tolerance tasks. Certainly not as fundamental as Spark, but critical in adopting the main idea to the file system context.


{% include twitter_plug.html %}
